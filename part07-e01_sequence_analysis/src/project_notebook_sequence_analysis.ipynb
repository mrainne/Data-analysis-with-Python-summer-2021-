{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence Analysis with Python\n",
    "\n",
    "Contact: Veli MÃ¤kinen veli.makinen@helsinki.fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following assignments introduce applications of hashing with ```dict()``` primitive of Python. While doing so, a rudimentary introduction to biological sequences is given. \n",
    "This framework is then enhanced with probabilities, leading to routines to generate random sequences under some constraints, including a general concept of *Markov-chains*. All these components illustrate the usage of ```dict()```, but at the same time introduce some other computational routines to efficiently deal with probabilities.   \n",
    "The function ```collections.defaultdict``` can be useful.\n",
    "\n",
    "Below are some \"suggested\" imports. Feel free to use and modify these, or not. Generally it's good practice to keep most or all imports in one place. Typically very close to the start of notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1063,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-08T22:04:22.831112Z",
     "start_time": "2019-07-08T22:04:22.688031Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from itertools import product\n",
    "\n",
    "import numpy as np\n",
    "from numpy.random import choice\n",
    "from scipy.stats import entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The automated TMC tests do not test cell outputs. These are intended to be evaluated in the peer reviews. So it is still be a good idea to make the outputs as clear and informative as possible.\n",
    "\n",
    "To keep TMC tests running as well as possible it is recommended to keep global variable assignments in the notebook to a minimum to avoid potential name clashes and confusion. Additionally you should keep all actual code exection in main guards to keep the test running smoothly. If you run [check_sequence.py](https://raw.githubusercontent.com/saskeli/data-analysis-with-python-summer-2019/master/check_outputs.py) in the `part07-e01_sequence_analysis` folder, the script should finish very quickly and optimally produce no output.\n",
    "\n",
    "If you download data from the internet during execution (codon usage table), the parts where downloading is done should not work if you decide to submit to the tmc server. Local tests should work fine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DNA and RNA\n",
    "\n",
    "A DNA molecule consist, in principle, of a chain of smaller molecules. These smaller molecules have some common basic components (bases) that repeat. For our purposes it is sufficient to know that these bases are nucleotides adenine, cytosine, guanine, and thymine with abbreviations ```A```, ```C```, ```G```, and ```T```. Given a *DNA sequence* e.g. ```ACGATGAGGCTCAT```, one can reverse engineer (with negligible loss of information) the corresponding DNA molecule.\n",
    "\n",
    "Parts of a DNA molecule can *transcribe* into an RNA molecule. In this process, thymine gets replaced by uracil (```U```). \n",
    "\n",
    "\n",
    "1. Write a function ```dna_to_rna``` to convert a given DNA sequence $s$ into an RNA sequence. For the sake of exercise, use ```dict()``` to store the symbol to symbol encoding rules. Create a program to test your function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-08T22:04:22.841952Z",
     "start_time": "2019-07-08T22:04:22.834721Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AACGUGAUUUC\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def dna_to_rna(s):\n",
    "    dna = 'ACGT'\n",
    "    rna = 'ACGU'\n",
    "    rules = dict(zip(dna, rna))\n",
    "    return \"\".join(rules[_] for _ in s)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    print(dna_to_rna(\"AACGTGATTTC\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Idea of solution\n",
    "Abbreviations for DNA and RNA are represented as strings. Each string contain all abbreviations. Then we create variable ```rules``` of type ```dict()``` using ```zip()``` function. We get the result looping through all characters in ```s``` and replacing each character according the ```rules```. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "The solution replaces ```T```s with ```U```s as it is supposed and other characters are not changed. Solution seems to work as required. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proteins\n",
    "\n",
    "Like DNA and RNA, protein molecule can be interpreted as a chain of smaller molecules, where the bases are now amino acids. RNA molecule may *translate* into a protein molecule, but instead of base by base, three bases of RNA correspond to one base of protein. That is, RNA sequence is read triplet (called codon) at a time. \n",
    "\n",
    "2. Consider the codon to amino acid conversion table in http://htmlpreview.github.io/?https://github.com/csmastersUH/data_analysis_with_python_2020/blob/master/Codon%20usage%20table.html. Write a function ```get_dict``` to read the table into a ```dict()```, such that for each RNA sequence of length 3, say $\\texttt{AGU}$, the hash table stores the conversion rule to the corresponding amino acid. You may store the html page to your local src directory,\n",
    "and parse that file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-08T22:04:22.867855Z",
     "start_time": "2019-07-08T22:04:22.845885Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'UUU': 'F', 'UCU': 'S', 'UAU': 'Y', 'UGU': 'C', 'UUC': 'F', 'UCC': 'S', 'UAC': 'Y', 'UGC': 'C', 'UUA': 'L', 'UCA': 'S', 'UAA': '*', 'UGA': '*', 'UUG': 'L', 'UCG': 'S', 'UAG': '*', 'UGG': 'W', 'CUU': 'L', 'CCU': 'P', 'CAU': 'H', 'CGU': 'R', 'CUC': 'L', 'CCC': 'P', 'CAC': 'H', 'CGC': 'R', 'CUA': 'L', 'CCA': 'P', 'CAA': 'Q', 'CGA': 'R', 'CUG': 'L', 'CCG': 'P', 'CAG': 'Q', 'CGG': 'R', 'AUU': 'I', 'ACU': 'T', 'AAU': 'N', 'AGU': 'S', 'AUC': 'I', 'ACC': 'T', 'AAC': 'N', 'AGC': 'S', 'AUA': 'I', 'ACA': 'T', 'AAA': 'K', 'AGA': 'R', 'AUG': 'M', 'ACG': 'T', 'AAG': 'K', 'AGG': 'R', 'GUU': 'V', 'GCU': 'A', 'GAU': 'D', 'GGU': 'G', 'GUC': 'V', 'GCC': 'A', 'GAC': 'D', 'GGC': 'G', 'GUA': 'V', 'GCA': 'A', 'GAA': 'E', 'GGA': 'G', 'GUG': 'V', 'GCG': 'A', 'GAG': 'E', 'GGG': 'G'}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def read_file_to_df():\n",
    "    '''Extract table data from html file and store it into a DataFrame'''\n",
    "    #filename = 'Codon_usage_table.html'\n",
    "    filename = 'src/Codon_usage_table.html' # test file needs src/ dictionary\n",
    "    with open (filename, 'r') as f:\n",
    "        content = f.read()\n",
    "        \n",
    "    # extract table from content string, content2 is list of tuples containing the data\n",
    "    content2 = re.findall(r'([ACGU]{3})\\s+([A-Z\\*]{1})\\s+(\\d\\.\\d+)\\s+(\\d+\\.\\d)\\s+\\(\\s*(\\d+)', content)\n",
    "\n",
    "    # create DataFrame from content2 list and name the columns as they are in original data\n",
    "    df_project = pd.DataFrame(content2, \n",
    "                              columns=['triplet', 'amino_acid', 'fraction', 'frequency', 'number']).astype({'fraction':float, 'frequency':float, 'number':int})\n",
    "    return df_project\n",
    "\n",
    "def get_dict():\n",
    "    df_project = read_file_to_df()\n",
    "    \n",
    "    d = dict(zip(df_project.triplet.to_list(), df_project.amino_acid.to_list()))\n",
    "    # print(len(d))\n",
    "    return d\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    codon_to_aa = get_dict()\n",
    "    print(codon_to_aa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Idea of solution\n",
    "\n",
    "File is stored locally to ```/src``` directory. File is read and table data is extracted from the content using regular expression. A ```DataFrame```is created from the extracted table data and columns are named as they are in original data. And data type of columns ```fraction``` and ```frequency```is set to ```float``` and ```number``` is set to ```int```. Dictionary is then created taking columns ```triplet``` and ```amino_acid``` and converting them to list and using ```zip``` to combine lists to a dictionary. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "Creating DataFrame was not required but it is easier and maybe also more efficient to read the table once than to read only part of data over and over again. Another option could be to read file contents and use regular expressions to extract desired data. Table has 5 columns and 64 rows. The result dictionary has 64 ```(key, value)``` pairs. Pairs should be correct because list representation of ```DataFrame``` column should have same order of items and merging lists with ```zip``` should not change the ordering of list items.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Use the same conversion table as above, but now write function `get_dict_list` to read the table into a `dict()`, such that for each amino acid the hash table stores the list of codons encoding it.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-08T22:04:22.882386Z",
     "start_time": "2019-07-08T22:04:22.872449Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'list'>, {'F': ['UUU', 'UUC'], 'S': ['UCU', 'UCC', 'UCA', 'UCG', 'AGU', 'AGC'], 'Y': ['UAU', 'UAC'], 'C': ['UGU', 'UGC'], 'L': ['UUA', 'UUG', 'CUU', 'CUC', 'CUA', 'CUG'], '*': ['UAA', 'UGA', 'UAG'], 'W': ['UGG'], 'P': ['CCU', 'CCC', 'CCA', 'CCG'], 'H': ['CAU', 'CAC'], 'R': ['CGU', 'CGC', 'CGA', 'CGG', 'AGA', 'AGG'], 'Q': ['CAA', 'CAG'], 'I': ['AUU', 'AUC', 'AUA'], 'T': ['ACU', 'ACC', 'ACA', 'ACG'], 'N': ['AAU', 'AAC'], 'K': ['AAA', 'AAG'], 'M': ['AUG'], 'V': ['GUU', 'GUC', 'GUA', 'GUG'], 'A': ['GCU', 'GCC', 'GCA', 'GCG'], 'D': ['GAU', 'GAC'], 'G': ['GGU', 'GGC', 'GGA', 'GGG'], 'E': ['GAA', 'GAG']})\n"
     ]
    }
   ],
   "source": [
    "def get_dict_list():\n",
    "    d = get_dict()\n",
    "    acids_d = defaultdict(list) \n",
    "    for key, value in d.items():\n",
    "        acids_d[value].append(key)\n",
    "    \n",
    "    #print(sum(map(len, acids_d.values())) == len(d))\n",
    "    return acids_d\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    aa_to_codons = get_dict_list()\n",
    "    print(aa_to_codons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Idea of solution\n",
    "\n",
    "Dictionary has already been constructed and there is no need to create it again. Since we are now interested in amino acids, we get values from the previous dictionary. Only unique amino acids are needed and we get them with ```set```. Then a new dictionary is created where keys are amino acids and value is an empty list. Now we can loop through items in the original dictionary, test if value is in set of amino acids and add triplet to correct list. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "Each amino acid has at least one triplet and value lists have same amount of items as the original dictionary. No amino acids or triplets are lost.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the conversion tables at hand, the following should be trivial to solve.\n",
    "\n",
    "4. Fill in function ```rna_to_prot``` in the stub solution to convert a given DNA sequence $s$ into a protein sequence. \n",
    "You may use the dictionaries from exercises 2 and 3. You can test your program with `ATGATATCATCGACGATGTAG`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-08T22:04:22.913321Z",
     "start_time": "2019-07-08T22:04:22.906646Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MISSTM*\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def rna_to_prot(s):\n",
    "    # partition s to triplets\n",
    "    s_triplets = (s[i:i+3] for i in range(0, len(s), 3))\n",
    "    triplet_d = get_dict()\n",
    "    prot = \"\".join(triplet_d[t] for t in s_triplets)\n",
    "    #print(len(prot) == len(s)/3)\n",
    "    return prot\n",
    "\n",
    "def dna_to_prot(s):\n",
    "    return rna_to_prot(dna_to_rna(s))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(dna_to_prot(\"ATGATATCATCGACGATGTAG\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Idea of solution\n",
    "\n",
    "The parameter string is partitioned into triplets. Dictionary with triplets as key is loaded into variable ```triplet_d``` and protein string is created looping through triplets and finding the amino acid from triplet dictionary. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "Lenght of original string is divisible by 3. If it wasn't, only ```len(s)//3``` triplets would be handled and that should be the length of the protein string. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may notice that there are $4^3=64$ different codons, but only 20 amino acids. That is, some triplets encode the same amino acid.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reverse translation\n",
    "\n",
    "It has been observed that among the codons coding the same amino acid, some are more frequent than others. These frequencies can be converted to probabilities. E.g. consider codons `AUU`, `AUC`, and `AUA` that code for amino acid isoleucine.\n",
    "If they are observed, say, 36, 47, 17 times, respectively, to code isoleucine in a dataset, the probability that a random such event is `AUU` $\\to$ isoleucine is 36/100.\n",
    "\n",
    "This phenomenon is called *codon adaptation*, and for our purposes it works as a good introduction to generation of random sequences under constraints.   \n",
    "\n",
    "5. Consider the codon adaptation frequencies in http://htmlpreview.github.io/?https://github.com/csmastersUH/data_analysis_with_python_2020/blob/master/Codon%20usage%20table.html and read them into a ```dict()```, such that for each RNA sequence of length 3, say `AGU`, the hash table stores the probability of that codon among codons encoding the same amino acid.\n",
    "Put your solution in the ```get_probabability_dict``` function. Use the column \"([number])\" to estimate the probabilities, as the two preceding columns contain truncated values.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-08T22:04:22.966173Z",
     "start_time": "2019-07-08T22:04:22.956013Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AAA: 0.434049\tAAC: 0.529633\tAAG: 0.565951\tAAU: 0.470367\tACA: 0.284188\tACC: 0.355232\n",
      "ACG: 0.113812\tACU: 0.246769\tAGA: 0.214658\tAGC: 0.239938\tAGG: 0.211091\tAGU: 0.149602\n",
      "AUA: 0.169062\tAUC: 0.469866\tAUG: 1.000000\tAUU: 0.361072\tCAA: 0.265017\tCAC: 0.581485\n",
      "CAG: 0.734983\tCAU: 0.418515\tCCA: 0.276603\tCCC: 0.323470\tCCG: 0.113196\tCCU: 0.286731\n",
      "CGA: 0.108812\tCGC: 0.183777\tCGG: 0.201554\tCGU: 0.080108\tCUA: 0.071380\tCUC: 0.195577\n",
      "CUG: 0.395702\tCUU: 0.131716\tGAA: 0.422453\tGAC: 0.535458\tGAG: 0.577547\tGAU: 0.464542\n",
      "GCA: 0.228121\tGCC: 0.399781\tGCG: 0.106176\tGCU: 0.265922\tGGA: 0.249922\tGGC: 0.337109\n",
      "GGG: 0.249882\tGGU: 0.163087\tGUA: 0.116577\tGUC: 0.238306\tGUG: 0.463346\tGUU: 0.181770\n",
      "UAA: 0.297019\tUAC: 0.556662\tUAG: 0.236738\tUAU: 0.443338\tUCA: 0.150517\tUCC: 0.217960\n",
      "UCG: 0.054398\tUCU: 0.187586\tUGA: 0.466243\tUGC: 0.543843\tUGG: 1.000000\tUGU: 0.456157\n",
      "UUA: 0.076568\tUUC: 0.535866\tUUG: 0.129058\tUUU: 0.464134\n"
     ]
    }
   ],
   "source": [
    "def get_probabability_dict():\n",
    "    project_df = read_file_to_df()\n",
    "    project_df['percentage'] = project_df.number / project_df.groupby('amino_acid')['number'].transform(sum) \n",
    "    #print(project_df.head())\n",
    "    \n",
    "    probability_dict = dict(zip(project_df.triplet.to_list(), project_df.percentage.to_list()))\n",
    "    return probability_dict\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    codon_to_prob = get_probabability_dict()\n",
    "    items = sorted(codon_to_prob.items(), key=lambda x: x[0])\n",
    "    for i in range(1 + len(items)//6):\n",
    "        print(\"\\t\".join(\n",
    "            f\"{k}: {v:.6f}\"\n",
    "            for k, v in items[i*6:6+i*6]\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Idea of solution\n",
    "\n",
    "Again we are handling the ```DataFrame```. Percentage is calculated by dividing column ```number```by number we get grouping ```DataFrame``` by amino acid and then calculate the sum of ```number```column for each amino acid. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "Number of items in list is again 64 as it should be. Printing first few rows of the DataFrame we can see the probability is approximately the same as the values in column ```frequncy```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you should have everything in place to easily solve the following.\n",
    "\n",
    "\n",
    "6. Write a class ```ProteinToMaxRNA``` with a ```convert``` method which converts a protein sequence into the most likely RNA sequence to be the source of this protein. Run your program with `LTPIQNRA`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-08T22:04:23.000743Z",
     "start_time": "2019-07-08T22:04:22.992108Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUGACCCCCAUCCAGAACAGAGCC\n"
     ]
    }
   ],
   "source": [
    "class ProteinToMaxRNA:\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def convert(self, s):\n",
    "        d = get_dict_list() # amino acid : triplets dict\n",
    "        p = get_probabability_dict() # triplet : probability dict\n",
    "        # for each amino acid in protein sequence find a triplet with maximum probability\n",
    "        # and add triplet to result\n",
    "        result = ''.join(max(zip(map(p.get, d[c]),d[c]))[1] for c in s)\n",
    "        \n",
    "        #print(len(s) == len(result)//3)\n",
    "        return result\n",
    "     \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    protein_to_rna = ProteinToMaxRNA()\n",
    "    print(protein_to_rna.convert(\"LTPIQNRA\"))\n",
    "    #print(protein_to_rna.convert(\"LTPIQ-RA\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Idea of solution\n",
    "\n",
    "In this solution we are using dictionaries from previous assignments. Using built-in functions we get the triplet with maximum probability and add it to ```result``` string. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "The solution can not handle cases when given protein sequence contains characters not found in amino acid dictionary. When sequence is valid the function should return a string which length is three times the lenght of protein sequence. Finding one-liner solution was not easy and required many iterations to get it work. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are almost ready to produce random RNA sequences that code a given protein sequence. For this, we need a subroutine to *sample from a probability distribution*. Consider our earlier example of probabilities 36/100, 47/100, and 17/100 for `AUU`, `AUC`, and `AUA`, respectively. \n",
    "Let us assume we have a random number generator ```random()``` that returns a random number from interval $[0,1)$. We may then partition the unit interval according to cumulative probabilities to $[0,36/100), [36/100,83/100), [83/100,1)$, respectively. Depending which interval the number ```random()``` hits, we select the codon accordingly.\n",
    "\n",
    "7. Write a function ```random_event``` that chooses a random event, given a probability distribution (set of events whose probabilities sum to 1).\n",
    "You can use function ```random.uniform``` to produce values uniformly at random from the range $[0,1)$. The distribution should be given to your function as a dictionary from events to their probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-08T22:04:23.036655Z",
     "start_time": "2019-07-08T22:04:23.030067Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C, C, C, T, C, T, C, T, T, T, T, C, G, C, T, C, C, T, C, A, G, T, T, C, T, T, C, C, C\n"
     ]
    }
   ],
   "source": [
    "def random_event(dist):\n",
    "    \"\"\"\n",
    "    Takes as input a dictionary from events to their probabilities.\n",
    "    Return a random event sampled according to the given distribution.\n",
    "    The probabilities must sum to 1.0\n",
    "    \"\"\"\n",
    "    # create dict where values are cumulative sum of dist values\n",
    "    d = dict(zip(dist.keys(), np.cumsum(list(dist.values()))))\n",
    "    #print(d)\n",
    "    rn = np.random.uniform(0, 1)\n",
    "    result = next(filter(lambda x: rn < d.get(x), d), '')\n",
    "    #print(result)\n",
    "    return result \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    distribution = dict(zip(\"ACGT\", [0.10, 0.35, 0.15, 0.40]))\n",
    "    print(\", \".join(random_event(distribution) for _ in range(29)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Idea of solution\n",
    "\n",
    "First we need to figure out how to handle the intervals and how to get cumulative probabilities. Cumulative probabilities can be counted with ```np.cumsum``` and that is the upper limit for the interval. We don't need to worry about lower limits.\n",
    "Random value in interval $[0, 1]$is generated using ```np.random'uniform```. And then we filter values where random value is less than dictionary value to find key. Function ```filter``` returns an iterator and we retrieve a value from iterator with ```next```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "It is not quite clear from the description what the function is supposed to return. This solition returns one of the letters $A, C, G, T$. Result is different every time the cell is run which suggests it might be somehow random. Letters $T$ and $C$ have higher probabilities and they seem to occur more often than the other letters. Not always but mostly. The solution has only been tested for given example and there might be some caveats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this general routine, the following should be easy to solve.\n",
    " \n",
    "8. Write a class ```ProteinToRandomRNA``` to produce a random RNA sequence encoding the input protein sequence according to the input codon adaptation probabilities. The actual conversion is done through the ```convert``` method. Run your program with `LTPIQNRA`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-08T22:04:23.073660Z",
     "start_time": "2019-07-08T22:04:23.067966Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUGACACCCAUCCAGAAUCGUGCG\n",
      "CUCACUCCGAUCCAAAAUAGAGCG\n",
      "CUGACUCCUAUUCAGAAUCGGGCC\n",
      "CUGACUCCAAUCCAGAACCGAGCU\n",
      "CUGACACCUAUUCAGAACCGAGCA\n",
      "CUCACUCCAAUUCAAAACAGGGCU\n",
      "UUAACGCCCAUCCAAAACCGCGCG\n",
      "UUGACCCCAAUUCAGAAUAGGGCC\n",
      "CUUACACCAAUUCAGAAUCGUGCG\n"
     ]
    }
   ],
   "source": [
    "class ProteinToRandomRNA(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def convert(self, s):\n",
    "        d = get_dict_list()\n",
    "        p = get_probabability_dict()\n",
    "        res = ''.join(random_event(dict(zip(d[c], map(p.get, d[c])))) for c in s)\n",
    "        \n",
    "        #print(len(res)//3 == len(s))\n",
    "        return res\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    protein_to_random_codons = ProteinToRandomRNA()\n",
    "    for _ in range(9):\n",
    "        print(protein_to_random_codons.convert(\"LTPIQNRA\"))\n",
    "    #for _ in range(3):\n",
    "    #    print(protein_to_random_codons.convert(\"WM\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Idea of solution\n",
    "\n",
    "First we need to create distribution dictionary combining probability dictionary with list dictionary, the principle is similar to assigment 6. When we have the distribution, we can use code in previous assignment to generate random RNA sequence. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "Since proteins are converted to triplets the length of result should be 3 times the length of the original sequence. Printing the result several times it is easier to see the variation in result and we can assume the is some randomness. Proteins W and M only have one coding option and solution should always return same value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating DNA sequences with higher-order Markov chains\n",
    "\n",
    "We will now reuse the machinery derived above in a related context. We go back to DNA sequences, and consider some easy statistics that can be used to characterize the sequences. \n",
    "First, just the frequencies of bases $\\texttt{A}$, $\\texttt{C}$, $\\texttt{G}$, $\\texttt{T}$ may reveal the species from which the input DNA originates; each species has a different base composition that has been formed during evolution. \n",
    "More interestingly, the areas where DNA to RNA transcription takes place (coding region) have an excess of $\\texttt{C}$ and $\\texttt{G}$ over $\\texttt{A}$ and $\\texttt{T}$. To detect such areas a common routine is to just use a *sliding window* of fixed size, say $k$, and compute for each window position \n",
    "$T[i..i+k-1]$ the base frequencies, where $T[1..n]$ is the input DNA sequence. When sliding the window from  $T[i..i+k-1]$ to $T[i+1..i+k]$ frequency $f(T[i])$ gets decreases by one and $f(T[i+k])$ gets increased by one. \n",
    "\n",
    "9. Write a *generator* ```sliding_window``` to compute sliding window base frequencies so that each moving of the window takes constant time. We saw in the beginning of the course one way how to create generators using\n",
    "  generator expression. Here we use a different way. For the function ```sliding_window``` to be a generator, it must have at least   one ```yield``` expression, see [https://docs.python.org/3/reference/expressions.html#yieldexpr](https://docs.python.org/3/reference/expressions.html#yieldexpr).\n",
    "  \n",
    "  Here is an example of a generator expression that works similarily to the built in `range` generator:\n",
    "  ```Python\n",
    "  def range(a, b=None, c=1):\n",
    "      current = 0 if b == None else a\n",
    "      end = a if b == None else b\n",
    "      while current < end:\n",
    "          yield current\n",
    "          current += c\n",
    "  ```\n",
    "  A yield expression can be used to return a value and *temporarily* return from the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1575,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-08T22:04:23.111365Z",
     "start_time": "2019-07-08T22:04:23.100858Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A': 0, 'C': 3, 'G': 0, 'T': 1}\n",
      "{'A': 0, 'C': 3, 'G': 1, 'T': 0}\n",
      "{'A': 1, 'C': 2, 'G': 1, 'T': 0}\n",
      "{'A': 1, 'C': 2, 'G': 1, 'T': 0}\n",
      "{'A': 1, 'C': 1, 'G': 2, 'T': 0}\n",
      "{'A': 1, 'C': 1, 'G': 2, 'T': 0}\n",
      "{'A': 0, 'C': 2, 'G': 2, 'T': 0}\n",
      "{'A': 0, 'C': 2, 'G': 2, 'T': 0}\n",
      "{'A': 0, 'C': 2, 'G': 1, 'T': 1}\n",
      "{'A': 0, 'C': 2, 'G': 0, 'T': 2}\n",
      "{'A': 0, 'C': 2, 'G': 0, 'T': 2}\n",
      "{'A': 0, 'C': 2, 'G': 0, 'T': 2}\n"
     ]
    }
   ],
   "source": [
    "def sliding_window(s, k):\n",
    "    \"\"\"\n",
    "    This function returns a generator that can be iterated over all\n",
    "    starting position of a k-window in the sequence.\n",
    "    For each starting position the generator returns the nucleotide frequencies\n",
    "    in the window as a dictionary.\n",
    "    \"\"\"\n",
    "    for i, _ in enumerate(s):\n",
    "        if i + k <= len(s):\n",
    "            yield dict(zip('ACGT', map(s[i:i+k].count, 'ACGT')))\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    s = \"TCCCGACGGCCTTCC\"\n",
    "    for d in sliding_window(s, 4):\n",
    "        print(d)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Idea of solution\n",
    "\n",
    "I couldn't figure out how to find solution with given for-loop and took liberty to find another way to solve the problem. String ```s``` needs to be partitioned into substrings of length ```k```, first substring starts from index 0, second from index 1 and so on, third from index 2 and so on. Then we count how many times each of the letters A, C, G, T is present in substring and ```yield``` dictionary where keys are letters mentioned above and values the count of letter in substring. Then we update starting position of the substring and repeat this until we reach index ```len(s)-k```. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "For given example string the solution prints $len(s) - k + 1 = 13$ dictionaries and if I have understood the description of the assignment correctly, they seem to be correct.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "Our models so far have been so-called *zero-order* models, as each event has been independent of other events. With sequences, the dependencies of events are naturally encoded by their *contexts*. Considering that a sequence is produced from left-to-right, a *first-order* context for $T[i]$ is $T[i-1]$, that is, the immediately preceding symbol. *First-order Markov chain* is a sequence produced by generating $c=T[i]$ with the probability of event of seeing symbol $c$ after previously generated symbol $a=T[i-1]$. The first symbol of the chain is sampled according to the zero-order model.  \n",
    "The first-order model can naturally be extended to contexts of length $k$, with $T[i]$ depending on $T[i-k..i-1]$. Then the first $k$ symbols of the chain are sampled according to the zero-order model.  The following assignments develop the routines to work with the *higher-order Markov chains*. \n",
    "In what follows, a $k$-mer is a substring $T[i..i+k-1]$ of the sequence at an arbitrary position. \n",
    "\n",
    "10. Write function ```context_list``` that given an input DNA sequence $T$ associates to each $k$-mer $W$ the concatenation of all symbols $c$ that appear after context $W$ in $T$, that is, $T[i..i+k]=Wc$. For example, <span style=\"color:red; font:courier;\">GA</span> is associated to <span style=\"color:blue; font: courier;\">TCT</span> in $T$=<span style=\"font: courier;\">AT<span style=\"color:red;\">GA</span><span style=\"color:blue;\">T</span>ATCATC<span style=\"color:red;\">GA</span><span style=\"color:blue;\">C</span><span style=\"color:red;\">GA</span><span style=\"color:blue;\">T</span>GTAG</span>, when $k=2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1558,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-08T22:04:23.168108Z",
     "start_time": "2019-07-08T22:04:23.162648Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'str'>, {'AT': 'GACCC', 'TG': 'A', 'GA': 'TCT', 'TA': 'TG', 'TC': 'AGT', 'CA': 'T', 'CG': 'AA', 'AC': 'G', 'CT': 'A'})\n"
     ]
    }
   ],
   "source": [
    "def context_list(s, k):\n",
    "    contexts = defaultdict(str)\n",
    "    try:\n",
    "        for i in range(len(s)):\n",
    "            subs = s[i:i+k]\n",
    "            window = next(sliding_window(s[i+k:], 1))\n",
    "            contexts[subs] += ''.join(filter(lambda x: window.get(x) != 0, window))\n",
    "    except StopIteration:\n",
    "        pass\n",
    "    return contexts\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    k = 2\n",
    "    s = \"ATGATATCATCGACGATCTAG\"\n",
    "    #print(len(s))\n",
    "    d = context_list(s, k)\n",
    "    print(d)\n",
    "    #print(len(d) <= len(s) - k)\n",
    "    #print(sum(map(len, d.values())) == len(s)-k)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Idea of solution\n",
    "\n",
    "Function ```sliding_window``` can be used in this one. First we create default dictionary of strings. Then looping though string ```s``` we get context, find next sliding window of length 1. Only one key has non-zero value and that key is added to context string."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "Length of result dictionary should be less or equal to ```len(s) - k```. The length of concatenated values of dictionary should be equal to ```len(s) - k```. Both of these requirements are met. Another option for solution could be regular expression to split ```s``` to smaller parts and handle them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. With the above solution, write function ```context_probabilities``` to count the frequencies of symbols in each context and convert these frequencies into probabilities. Run `context_probabilities` with $T=$ `ATGATATCATCGACGATGTAG` and $k$ values 0 and 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1907,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-08T22:04:23.218964Z",
     "start_time": "2019-07-08T22:04:23.213773Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 0 : Context probabilities: {'': {'A': 0.3333333333333333, 'T': 0.2857142857142857, 'G': 0.23809523809523808, 'C': 0.14285714285714285}}\n",
      "k = 2 : Context probabilities: {'AT': {'G': 0.4, 'A': 0.2, 'C': 0.4}, 'TG': {'A': 0.5, 'T': 0.5}, 'GA': {'T': 0.6666666666666666, 'C': 0.3333333333333333}, 'TA': {'T': 0.5, 'G': 0.5}, 'TC': {'A': 0.5, 'G': 0.5}, 'CA': {'T': 1.0}, 'CG': {'A': 1.0}, 'AC': {'G': 1.0}, 'GT': {'A': 1.0}}\n"
     ]
    }
   ],
   "source": [
    "def context_probabilities(s, k):\n",
    "    context_l = context_list(s, k)\n",
    "    context_p = defaultdict(dict)\n",
    "    \n",
    "    for key, val in context_l.items():\n",
    "        for val_key in val:\n",
    "            context_p[key][val_key] = val.count(val_key)/len(val) \n",
    "    return context_p\n",
    "\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    s = 'ATGATATCATCGACGATGTAG'\n",
    "    for k in [0,2]:\n",
    "        print('k = {} : Context probabilities: {}'.format(k, dict(context_probabilities(s, k))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Idea of solution\n",
    "\n",
    "Get context list, loop through list and create dictionary of letters and their frequencies and set it value of k-mer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "Description of assignment does not define how the frequencies should be presented. Dictionary is chosen because it is quite easy to read and interpret. Frequencies seem ok and solution passes the tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. With the above solution and the function ```random_event``` from the earlier exercise, write class ```MarkovChain```. Its ```generate``` method should generate a random DNA sequence following the original $k$-th order Markov chain probabilities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1917,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-08T22:04:23.279315Z",
     "start_time": "2019-07-08T22:04:23.253983Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TGTATGATGA\n"
     ]
    }
   ],
   "source": [
    "class MarkovChain:\n",
    "    \n",
    "    def __init__(self, zeroth, kth, k=2):\n",
    "        self.k = k\n",
    "        self.zeroth = zeroth\n",
    "        self.kth = kth\n",
    "        \n",
    "    def generate(self, n, seed=None):\n",
    "        np.random.seed(seed) # function documentation does not recommend usage!\n",
    "        # just in case these are needed\n",
    "        k = self.k if self.k < n else n\n",
    "        zeroth = self.zeroth\n",
    "        kth = self.kth\n",
    "        # create random sequence of length k based on zeroth\n",
    "        #print(zeroth)\n",
    "        s = ''.join(random_event(zeroth) for _ in range(k))\n",
    "        for _ in range(n-k):\n",
    "            next_item = s[-k:]\n",
    "            s += random_event(kth[next_item])\n",
    "        return s    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    zeroth = {'A': 0.2, 'C': 0.19, 'T': 0.31, 'G': 0.3}\n",
    "    kth = {'GT': {'A': 1.0, 'C': 0.0, 'T': 0.0, 'G': 0.0},\n",
    "           'CA': {'A': 0.0, 'C': 0.0, 'T': 1.0, 'G': 0.0},\n",
    "           'TC': {'A': 0.5, 'C': 0.0, 'T': 0.0, 'G': 0.5},\n",
    "           'GA': {'A': 0.0, 'C': 0.3333333333333333, 'T': 0.6666666666666666, 'G': 0.0},\n",
    "           'TG': {'A': 0.5, 'C': 0.0, 'T': 0.5, 'G': 0.0},\n",
    "           'AT': {'A': 0.2, 'C': 0.4, 'T': 0.0, 'G': 0.4},\n",
    "           'TA': {'A': 0.0, 'C': 0.0, 'T': 0.5, 'G': 0.5},\n",
    "           'AC': {'A': 0.0, 'C': 0.0, 'T': 0.0, 'G': 1.0},\n",
    "           'CG': {'A': 1.0, 'C': 0.0, 'T': 0.0, 'G': 0.0}}\n",
    "    n = 10    \n",
    "    seed = 0\n",
    "    mc = MarkovChain(zeroth, kth)\n",
    "    print(mc.generate(n, seed))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Idea of solution\n",
    "\n",
    "Description of problem is somewhat vague, so it is unclear what is actually required.\n",
    "\n",
    "First we set variables, then we generate first $k$ items in sequence from ```zeroth``` and the rest of the items are generated from ```kth``` using function ```random_event```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "Solution produces a KeyError but passes the tests. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have survived so far without problems, please run your program a few more times with different inputs. At some point you should get a lookup error in your hash-table! The reason for this is not your code, but the way we defined the model: Some $k$-mers may not be among the training data (input sequence $T$), but such can be generated as the first $k$-mer that is generated using the zero-order model.  \n",
    "\n",
    "A general approach to fixing such issues with incomplete training data is to use *pseudo counts*. That is, all imaginable events are initialized to frequency count 1.   \n",
    "\n",
    "13. Write a new solution `context_pseudo_probabilities` based on the solution to problem 11. But this time use pseudo counts in order to obtain a $k$-th order Markov chain that can assign a probability for any DNA sequence. You may use the standard library function `itertools.product` to iterate over all $k$-mer of given length (`product(\"ACGT\", repeat=k)`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-08T22:04:23.303566Z",
     "start_time": "2019-07-08T22:04:23.296028Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zeroth: {'A': 0.32, 'C': 0.16, 'G': 0.24, 'T': 0.28}\n",
      "AA: {'A': 0.25, 'C': 0.25, 'G': 0.25, 'T': 0.25}\n",
      "AC: {'A': 0.2, 'C': 0.2, 'G': 0.4, 'T': 0.2}\n",
      "AG: {'A': 0.25, 'C': 0.25, 'G': 0.25, 'T': 0.25}\n",
      "AT: {'A': 0.2222222222222222, 'C': 0.3333333333333333, 'G': 0.3333333333333333, 'T': 0.1111111111111111}\n",
      "CA: {'A': 0.2, 'C': 0.2, 'G': 0.2, 'T': 0.4}\n",
      "CC: {'A': 0.25, 'C': 0.25, 'G': 0.25, 'T': 0.25}\n",
      "CG: {'A': 0.5, 'C': 0.16666666666666666, 'G': 0.16666666666666666, 'T': 0.16666666666666666}\n",
      "CT: {'A': 0.25, 'C': 0.25, 'G': 0.25, 'T': 0.25}\n",
      "GA: {'A': 0.14285714285714285, 'C': 0.2857142857142857, 'G': 0.14285714285714285, 'T': 0.42857142857142855}\n",
      "GC: {'A': 0.25, 'C': 0.25, 'G': 0.25, 'T': 0.25}\n",
      "GG: {'A': 0.25, 'C': 0.25, 'G': 0.25, 'T': 0.25}\n",
      "GT: {'A': 0.4, 'C': 0.2, 'G': 0.2, 'T': 0.2}\n",
      "TA: {'A': 0.16666666666666666, 'C': 0.16666666666666666, 'G': 0.3333333333333333, 'T': 0.3333333333333333}\n",
      "TC: {'A': 0.3333333333333333, 'C': 0.16666666666666666, 'G': 0.3333333333333333, 'T': 0.16666666666666666}\n",
      "TG: {'A': 0.3333333333333333, 'C': 0.16666666666666666, 'G': 0.16666666666666666, 'T': 0.3333333333333333}\n",
      "TT: {'A': 0.25, 'C': 0.25, 'G': 0.25, 'T': 0.25}\n",
      "\n",
      " TGAGACTGCTATCCTATCGT\n"
     ]
    }
   ],
   "source": [
    "def context_pseudo_probabilities(s, k):\n",
    "    k_mers = (''.join(item) for item in product('ACGT', repeat=k))\n",
    "    context_l = {**{k: 'ACGT' for k in k_mers}, **context_list(s, k)}\n",
    "    context_p = defaultdict(dict)\n",
    "    \n",
    "    for key, value in context_l.items():\n",
    "        for c in 'ACGT':\n",
    "            context_p[key][c] = (value.count(c)+1)/(len(value)+4)  \n",
    "            \n",
    "    return context_p\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    k = 2\n",
    "    s = \"ATGATATCATCGACGATGTAG\"\n",
    "    kth = context_pseudo_probabilities(s, k)\n",
    "    zeroth = context_pseudo_probabilities(s, 0)[\"\"]\n",
    "    print(f\"zeroth: {zeroth}\")\n",
    "    print(\"\\n\".join(f\"{k}: {dict(v)}\" for k, v in kth.items()))\n",
    "    \n",
    "    print(\"\\n\", MarkovChain(zeroth, kth, k).generate(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Idea of solution\n",
    "\n",
    "Still not sure what is supposed to be done. Finding all k-mers with ```product```. Create new ```dict``` where all k-mers are present. Get original dictionary and update it with the new one to add missing keys. Probabilities are counted adding 1 to each character count and 4 to value length. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "Solution passes the tests but I still don't quite understand why this works but other options didn't."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14. Write class ```MarkovProb``` that given the $k$-th order Markov chain developed above to the constructor, its method ```probability``` computes the probability of a given input DNA sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-08T22:04:23.346222Z",
     "start_time": "2019-07-08T22:04:23.330779Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of sequence ATGATATCATCGACGATGTAG is 2.831270190340017e-10\n"
     ]
    }
   ],
   "source": [
    "class MarkovProb:\n",
    "    def __init__(self, k, zeroth, kth):\n",
    "        self.k = k\n",
    "        self.zeroth = zeroth\n",
    "        self.kth = kth\n",
    "        \n",
    "    def probability(self, s):\n",
    "        k = self.k if self.k < len(s) else len(s)\n",
    "        kth = self.kth\n",
    "        zeroth = self.zeroth\n",
    "        res = []\n",
    "        k_mer = s[:k]\n",
    "        res.extend([zeroth[s[i]] for i in range(k)])\n",
    "\n",
    "        for c in s[k:]:\n",
    "            res.append(kth[k_mer][c])\n",
    "            k_mer = k_mer[1:]+c\n",
    "\n",
    "        return np.prod(res)\n",
    "\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    k = 2\n",
    "    kth = context_pseudo_probabilities(\"ATGATATCATCGACGATGTAG\", k)\n",
    "    zeroth = context_pseudo_probabilities(\"ATGATATCATCGACGATGTAG\", 0)[\"\"]\n",
    "    mc = MarkovProb(2, zeroth, kth)\n",
    "    s=\"ATGATATCATCGACGATGTAG\"\n",
    "    print(f\"Probability of sequence {s} is {mc.probability(s)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Idea of solution\n",
    "\n",
    "Get first $k$ elements and their probabilities from ```zeroth``` and then rest of the probabilities from ```kth```. Cases where length of parameter string is less than k are handled by setting ```k``` to ```len(s)``` if string is shorter than k. In that case all values are taken from ```zeroth```. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "Solution passes all tests. It's not pretty but does what is asked. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the last assignment you might end up in trouble with precision, as multiplying many small probabilities gives a really small number in the end. There is an easy fix by using so-called log-transform. \n",
    "Consider computation of $P=s_1 s_2 \\cdots s_n$, where $0\\leq s_i\\leq 1$ for each $i$. Taking logarithm in base 2 from both sides gives $\\log _2 P= \\log _2 (s_1 s_2 \\cdots s_n)=\\log_2 s_1 + \\log_2 s_2 + \\cdots \\log s_n= \\sum_{i=1}^n \\log s_i$, with repeated application of the property that the logarithm of a multiplication of two numbers is the sum of logarithms of the two numbers taken separately. The results is abbreviated as log-probability.\n",
    "\n",
    "15. Write class ```MarkovLog``` that given the $k$-th order Markov chain developed above to the constructor, its method ```log_probability``` computes the log-probability of a given input DNA sequence. Run your program with $T=$ `ATGATATCATCGACGATGTAG` and $k=2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-08T22:04:23.390453Z",
     "start_time": "2019-07-08T22:04:23.379760Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log probability of sequence ATGATATCATCGACGATGTAG is -31.717831515538307\n"
     ]
    }
   ],
   "source": [
    "class MarkovLog(object):\n",
    "\n",
    "    def __init__(self, k, zeroth, kth):\n",
    "        self.k = k\n",
    "        self.zeroth = zeroth\n",
    "        self.kth = kth\n",
    "        \n",
    "    def log_probability(self, s):\n",
    "        return np.log2(MarkovProb.probability(self, s))\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    k = 2\n",
    "    kth = context_pseudo_probabilities(\"ATGATATCATCGACGATGTAG\", k)\n",
    "    zeroth = context_pseudo_probabilities(\"ATGATATCATCGACGATGTAG\", 0)[\"\"]\n",
    "    mc = MarkovLog(2, zeroth, kth)\n",
    "    s=\"ATGATATCATCGACGATGTAG\"\n",
    "    print(f\"Log probability of sequence {s} is {mc.log_probability(s)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Idea of solution\n",
    "\n",
    "We take previous solution and use ```np.log2``` to get log-probability. There is also need to fill in missing pieces in ```MarkovLog.__init__```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "Solution passes all tests. Result is negative because ```log2(x)``` is negative when $0\\leq x \\leq1$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, if you try to use the code so far for very large inputs, you might observe that the concatenation of symbols following a context occupy considerable amount of space. This is unnecessary, as we only need the frequencies. \n",
    "\n",
    "16. Optimize the space requirement of your code from exercise 13 for the $k$-th order Markov chain by replacing the concatenations by direct computations of the frequencies. Implement this as the\n",
    "  ```better_context_probabilities``` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-08T22:04:23.422302Z",
     "start_time": "2019-07-08T22:04:23.416330Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AA: {'A': 0.25, 'C': 0.25, 'G': 0.25, 'T': 0.25}\n",
      "AC: {'A': 0.2, 'C': 0.2, 'G': 0.4, 'T': 0.2}\n",
      "AG: {'A': 0.25, 'C': 0.25, 'G': 0.25, 'T': 0.25}\n",
      "AT: {'A': 0.2222222222222222, 'C': 0.3333333333333333, 'G': 0.3333333333333333, 'T': 0.1111111111111111}\n",
      "CA: {'A': 0.2, 'C': 0.2, 'G': 0.2, 'T': 0.4}\n",
      "CC: {'A': 0.25, 'C': 0.25, 'G': 0.25, 'T': 0.25}\n",
      "CG: {'A': 0.5, 'C': 0.16666666666666666, 'G': 0.16666666666666666, 'T': 0.16666666666666666}\n",
      "CT: {'A': 0.25, 'C': 0.25, 'G': 0.25, 'T': 0.25}\n",
      "GA: {'A': 0.14285714285714285, 'C': 0.2857142857142857, 'G': 0.14285714285714285, 'T': 0.42857142857142855}\n",
      "GC: {'A': 0.25, 'C': 0.25, 'G': 0.25, 'T': 0.25}\n",
      "GG: {'A': 0.25, 'C': 0.25, 'G': 0.25, 'T': 0.25}\n",
      "GT: {'A': 0.4, 'C': 0.2, 'G': 0.2, 'T': 0.2}\n",
      "TA: {'A': 0.16666666666666666, 'C': 0.16666666666666666, 'G': 0.3333333333333333, 'T': 0.3333333333333333}\n",
      "TC: {'A': 0.3333333333333333, 'C': 0.16666666666666666, 'G': 0.3333333333333333, 'T': 0.16666666666666666}\n",
      "TG: {'A': 0.3333333333333333, 'C': 0.16666666666666666, 'G': 0.16666666666666666, 'T': 0.3333333333333333}\n",
      "TT: {'A': 0.25, 'C': 0.25, 'G': 0.25, 'T': 0.25}\n"
     ]
    }
   ],
   "source": [
    "def better_context_probabilities(s, k):\n",
    "    k_mers = (''.join(item) for item in product('ACGT', repeat=k+1))\n",
    "    better_prob = defaultdict(dict)\n",
    "    for key in k_mers:\n",
    "        count_key = s.count(key) + 1\n",
    "        count_total = s.count(key[:k])+ (3 if s.endswith(key[:k]) else 4)\n",
    "        better_prob[key[:k]][key[-1]] = count_key/count_total\n",
    "    return better_prob\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    k = 2\n",
    "    s = \"ATGATATCATCGACGATGTAG\"\n",
    "    d = better_context_probabilities(s, k)\n",
    "    print(\"\\n\".join(f\"{k}: {v}\" for k, v in d.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Idea of solution\n",
    "\n",
    "First we create k+1-mers. Then we loop through them and count pseudo probabilities for each k-mer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "Solution passes the tests. Not sure if it is any better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the earlier approach of explicit concatenation of symbols following a context suffered from inefficient use of space, it does have a benefit of giving another much simpler strategy to sample from the distribution: \n",
    "observe that an element of the concatenation taken uniformly randomly is sampled exactly with the correct probability. \n",
    "\n",
    "17. Revisit the solution 12 and modify it to directly sample from the concatenation of symbols following a context. The function ```np.random.choice``` may be convenient here. Implement the modified version as the new `SimpleMarkovChain` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1919,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-08T22:04:23.462556Z",
     "start_time": "2019-07-08T22:04:23.453101Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATATCGATAT\n"
     ]
    }
   ],
   "source": [
    "class SimpleMarkovChain(object):\n",
    "    def __init__(self, s, k):\n",
    "        self.k = k\n",
    "        self.s = s\n",
    "        \n",
    "\n",
    "    def generate(self, n, seed=None):\n",
    "        s = self.s\n",
    "        k = self.k if self.k < n else n\n",
    "        np.random.seed(seed)\n",
    "        context = context_list(s, k)\n",
    "        # generate first items choosing from k items from s\n",
    "        result = np.random.choice(list(s), k)\n",
    "        for _ in range(n-k):\n",
    "            k_mer = ''.join(result[-k:])\n",
    "            next_item = np.random.choice(list(context[k_mer])) if context[k_mer] != '' else np.random.choice(list(s))\n",
    "\n",
    "            result = np.append(result, next_item)\n",
    "        return ''.join(result)\n",
    "    \n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    k = 2\n",
    "    s = \"ATGATATCATCGACGATGTAG\"\n",
    "    n = 10\n",
    "    seed = 7\n",
    "    mc = SimpleMarkovChain(s, k)\n",
    "    print(mc.generate(n, seed))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Idea of solution\n",
    "\n",
    "Get ```context_list```. Select first $k$ items from ```s```. Rest of characters are selected from contexts.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "Once again I have no idea what is supposed to be done but with some tinkering managed to pass the tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $k$-mer index\n",
    "\n",
    "Our $k$-th order Markov chain can now be modified to a handy index structure called $k$-mer index. This index structure associates to each $k$-mer its list of occurrence positions in DNA sequence $T$.  Given a query $k$-mer $W$, one can thus easily list all positions $i$ with  $T[i..k-1]=W$.\n",
    "\n",
    "18. Implement function ```kmer_index``` inspired by your earlier code for the $k$-th order Markov chain. Test your program with `ATGATATCATCGACGATGTAG` and $k=2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1562,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-08T22:04:23.504405Z",
     "start_time": "2019-07-08T22:04:23.494537Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using string:\n",
      "ATGATATCATCGACGATGTAG\n",
      "012345678901234567890\n",
      "\n",
      "2-mer index is:\n",
      "{'AT': [0, 3, 5, 8, 15], 'TG': [1, 16], 'GA': [2, 11, 14], 'TA': [4, 18], 'TC': [6, 9], 'CA': [7], 'CG': [10, 13], 'AC': [12], 'GT': [17], 'AG': [19]}\n"
     ]
    }
   ],
   "source": [
    "def kmer_index(s, k):\n",
    "    kmers = re.finditer(r'(?=(\\w{'+str(k)+r'}))', s)\n",
    "    indices = defaultdict(list)\n",
    "    for m in kmers:\n",
    "        indices[m[1]].append(m.start())\n",
    "    return indices\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    k=2\n",
    "    s = \"ATGATATCATCGACGATGTAG\"\n",
    "    print(\"Using string:\")\n",
    "    print(s)\n",
    "    print(\"\".join([str(i%10) for i in range(len(s))]))\n",
    "    print(f\"\\n{k}-mer index is:\")\n",
    "    d=kmer_index(s, k)\n",
    "    print(dict(d))\n",
    "    #print(len(s)-k+1 == sum(len(item) for item in d.values()))\n",
    "    #print(sorted(i for item in d.values() for i in item) == list(range(len(s)-k+1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Idea of solution\n",
    "\n",
    "Find k-mers with regular expression and create defaultdict where keys are k-mers and values list of indices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "There should be ```len(s)-k+1``` indices and all indices in range $[0, len(s)-k+1]$ should be present. Tests pass. Regular expression is needed to find overlapping k-mers. Maybe there is more efficient solution but I cannot figure it out. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of probability distributions\n",
    "\n",
    "Now that we know how to learn probability distributions from data, we might want to compare two such distributions, for example, to test if our programs work as intended. \n",
    "\n",
    "Let $P=\\{p_1,p_2,\\ldots, p_n\\}$ and $Q=\\{q_1,q_2,\\ldots, q_n\\}$ be two probability distributions for the same set of $n$ events. This means $\\sum_{i=1}^n p_i=\\sum_{i=1}^n q_i=1$, $0\\leq p_j \\leq 1$, and $0\\leq q_j \\leq 1$ for each event $j$. \n",
    "\n",
    "*Kullback-Leibler divergence* is a measure $d()$ for the *relative entropy* of $P$ with respect to $Q$ defined as \n",
    "$d(P||Q)=\\sum_{i=1}^n p_i \\log\\frac{p_i}{q_i}$.\n",
    "\n",
    "\n",
    "This measure is always non-negative, and 0 only when $P=Q$. It can be interpreted as the gain of knowing $Q$ to encode $P$. Note that this measure is not symmetric.\n",
    "\n",
    "19. Write function ```kullback_leibler``` to compute $d(P||Q)$. Test your solution by generating a random RNA sequence\n",
    "  encoding the input protein sequence according to the input codon adaptation probabilities.\n",
    "  Then you should learn the codon adaptation probabilities from the RNA sequence you generated.\n",
    "  Then try the same with uniformly random RNA sequences (which don't have to encode any\n",
    "  specific protein sequence). Compute the relative entropies between the\n",
    "  three distribution (original, predicted, uniform) and you should observe a clear difference.\n",
    "  Because $d(P||Q)$ is not symmetric, you can either print both $d(P||Q)$ and $d(Q||P)$,\n",
    "  or their average.\n",
    "  \n",
    "  This problem may be fairly tricky. Only the `kullback_leibler` function is automatically tested. The codon probabilities is probably a useful helper function. The main guarded section can be completed by filling out the `pass` sections using tooling from previous parts and fixing the *placeholder* lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1862,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-08T22:04:23.557340Z",
     "start_time": "2019-07-08T22:04:23.539188Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "d(original || predicted) = 0.2201957647081663\n",
      "d(predicted || original) = 0.19783887203478662\n",
      "\n",
      "d(original || uniform) = 0.2688377920407011\n",
      "d(uniform || original) = 0.26123719816813823\n",
      "\n",
      "d(predicted || uniform) = 0.06371516303733828\n",
      "d(uniform || predicted) = 0.06538513270350868\n"
     ]
    }
   ],
   "source": [
    "def codon_probabilities(rna):\n",
    "    \"\"\"\n",
    "    Given an RNA sequence, simply calculates the proability of\n",
    "    all 3-mers empirically based on the sequence\n",
    "    \"\"\"\n",
    "    return {\"\".join(codon): rna.count(''.join(codon))/len(rna) for codon in product(\"ACGU\", repeat=3)}\n",
    "    \n",
    "def kullback_leibler(p, q):\n",
    "    \"\"\"\n",
    "    Computes Kullback-Leibler divergence between two distributions.\n",
    "    Both p and q must be dictionaries from events to probabilities.\n",
    "    The divergence is defined only when q[event] == 0 implies p[event] == 0.\n",
    "    \"\"\"\n",
    "    \n",
    "    p_arr = np.fromiter(p.values(), dtype=float)\n",
    "    q_arr = np.fromiter(q.values(), dtype=float)\n",
    "    try:\n",
    "        # this part is only for raising ZeroDivisionError, only needed to pass the tests\n",
    "        if np.any(q_arr==0):\n",
    "            1/0\n",
    "        return np.sum(np.where((p_arr != 0) & (q_arr != 0), p_arr * np.log2(p_arr/q_arr), 0))\n",
    "    except ZeroDivisionError:\n",
    "        raise\n",
    "    \n",
    "    #return entropy(p_arr, q_arr, base=2)#np.sum(np.where((p_arr != 0) & (q_arr != 0), p_arr * (np.log2(p_arr) - np.log2(q_arr)), 0))\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    aas = list(\"*ACDEFGHIKLMNPQRSTVWY\") # List of amino acids\n",
    "    n = 10000\n",
    "    \n",
    "    # generate a random protein and some associated rna\n",
    "    protein = \"\".join(choice(aas, n))\n",
    "    protein_to_random_rna = ProteinToRandomRNA()\n",
    "    rnd_rna = protein_to_random_rna.convert(protein)\n",
    "    #pass\n",
    "    # Maybe check that converting back to protein results in the same sequence\n",
    "    print(rna_to_prot(rnd_rna) == protein)\n",
    "\n",
    "    \n",
    "    # Calculate codon probabilities of the rna sequence\n",
    "    cp_predicted = codon_probabilities(rnd_rna) # placeholder call\n",
    "    \n",
    "    # Calculate codon probabilities based on the codon usage table\n",
    "    df = read_file_to_df()\n",
    "    total_number = 40662582# from table\n",
    "    df['frequency'] = df.number/total_number\n",
    "    d = df[['triplet','frequency']].sort_values('triplet', ascending=True).to_dict('list')\n",
    "    cp_orig = dict(zip(d['triplet'], d['frequency']))\n",
    "    \n",
    "    # Create a completely random RNA sequence and get the codon probabilities\n",
    "    #pass\n",
    "    dist = dict(zip('ACGU', [0.25]*4))\n",
    "    rnd_rna2 = ''.join(random_event(dist) for _ in range(10000))\n",
    "    #mc = SimpleMarkovChain(''.join(choice(list('ACGU'), n)), 3)\n",
    "    #rnd_rna2 = mc.generate(n)\n",
    "    cp_uniform = codon_probabilities(rnd_rna2.replace('T', 'U')) # placeholder call\n",
    "    \n",
    "    print(\"d(original || predicted) =\", kullback_leibler(cp_orig, cp_predicted))\n",
    "    print(\"d(predicted || original) =\", kullback_leibler(cp_predicted, cp_orig))\n",
    "    print()\n",
    "    print(\"d(original || uniform) =\", kullback_leibler(cp_orig, cp_uniform))\n",
    "    print(\"d(uniform || original) =\", kullback_leibler(cp_uniform, cp_orig))\n",
    "    print()\n",
    "    print(\"d(predicted || uniform) =\", kullback_leibler(cp_predicted, cp_uniform))\n",
    "    print(\"d(uniform || predicted) =\", kullback_leibler(cp_uniform, cp_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Idea of solution\n",
    "\n",
    "Calculation of Kullback-Leibner is done by given formula and is pretty straighforward. However, a little trick to raise the exception. Generating sequences and calculating probabilities is just guessing what should be done. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "Description of assignment says that there should be clear differences in Kullback-Leibner divergences but it is not said what is **clear difference** is it . Predicted/uniform pair is closest to zero. All values are non-negative as they should be and they are also less than 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stationary and equilibrium distributions (extra)\n",
    "\n",
    "Let us consider a Markov chain of order one on the set of nucleotides.\n",
    "Its transition probabilities can be expressed as a $4 \\times 4$ matrix\n",
    "$P=(p_{ij})$, where the element $p_{ij}$ gives the probability of the $j$th nucleotide\n",
    "on the condition the previous nucleotide was the $i$th. An example of a transition matrix\n",
    "is\n",
    "\n",
    "\\begin{array}{l|rrrr}\n",
    " &     A &    C &     G &    T \\\\\n",
    "\\hline\n",
    "A &  0.30 &  0.0 &  0.70 &  0.0 \\\\\n",
    "C &  0.00 &  0.4 &  0.00 &  0.6 \\\\\n",
    "G &  0.35 &  0.0 &  0.65 &  0.0 \\\\\n",
    "T &  0.00 &  0.2 &  0.00 &  0.8 \\\\\n",
    "\\end{array}.\n",
    "\n",
    "A distribution $\\pi=(\\pi_1,\\pi_2,\\pi_3,\\pi_4)$ is called *stationary*, if\n",
    "$\\pi = \\pi P$ (the product here is matrix product).\n",
    "\n",
    "20. Write function ```get_stationary_distributions``` that gets a transition matrix as parameter,\n",
    "  and returns the list of stationary distributions. You can do this with NumPy by\n",
    "  first taking transposition of both sides of the above equation to get equation\n",
    "  $\\pi^T = P^T \\pi^T$. Using numpy.linalg.eig take all eigenvectors related to\n",
    "  eigenvalue 1.0. By normalizing these vectors to sum up to one get the stationary distributions\n",
    "  of the original transition matrix. In the ```main``` function print the stationary distributions\n",
    "  of the above transition matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1781,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-08T22:04:23.591644Z",
     "start_time": "2019-07-08T22:04:23.580588Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+0.333, +0.000, +0.667, +0.000\n",
      "-0.000, +0.250, -0.000, +0.750\n"
     ]
    }
   ],
   "source": [
    "def get_stationary_distributions(transition):\n",
    "    \"\"\"\n",
    "    The function get a transition matrix of a degree one Markov chain as parameter.\n",
    "    It returns a list of stationary distributions, in vector form, for that chain.\n",
    "    \"\"\"\n",
    "    eigenvalues, eigenvectors = np.linalg.eig(transition.T)\n",
    "    # find indices of eigenvalues vector, where eigenvalue close to 1\n",
    "    # where returns tuple and we are interested only in first part of it hence [0]\n",
    "    idx = np.where(np.isclose(eigenvalues, 1))[0]\n",
    "    # pick corresponding eigenvectors and transpose the array (matrix)\n",
    "    pi = eigenvectors[:, idx].T\n",
    "    # calculate stationary distribution, \n",
    "    stationary = pi/pi.sum(axis=1, keepdims=True)\n",
    "    return stationary  \n",
    "    \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    transition=np.array([[0.3, 0, 0.7, 0],\n",
    "                         [0, 0.4, 0, 0.6],\n",
    "                         [0.35, 0, 0.65, 0],\n",
    "                         [0, 0.2, 0, 0.8]])\n",
    "    print(\"\\n\".join(\n",
    "        \", \".join(f\"{pv:+.3f}\" for pv in p)\n",
    "    for p in get_stationary_distributions(transition)))\n",
    "    #d = get_stationary_distributions(transition)\n",
    "    #print(d.dot(transition))\n",
    "    #print(np.sum(d, axis=1))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Idea of solution\n",
    "\n",
    "First we get eigenvalues and eigenvectors with ```np.linalg.eig```, since we need left eigenvectors instead of right, transition matrix needs to be transposed. Then we find from eigenvalues array indices of values close to 1.0 and then pick those columns from eigenvectors. Then we calculate the stationary matrix dividing each value in matrix by the row sum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "We can check that dot product of stationary distribution and transition matrix is equal to distribution ($\\pi = \\pi P$). We can also check row sums of stationary distribution equal 1.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "21. Implement the `kl_divergence` function below so that the main guarded code runs properly. Using your modified Markov chain generator generate a nucleotide sequence $s$ of length $10\\;000$. Choose prefixes of $s$ of lengths $1, 10, 100, 1000$, and $10\\;000$. For each of these prefixes find out their nucleotide distribution (of order 0) using your earlier tool. Use 1 as the pseudo count. Then, for each prefix, compute the KL divergence between the initial distribution and the normalized nucleotide distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1884,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-08T22:04:23.635060Z",
     "start_time": "2019-07-08T22:04:23.618890Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transition probabilities are:\n",
      "[[0.3  0.   0.7  0.  ]\n",
      " [0.   0.4  0.   0.6 ]\n",
      " [0.35 0.   0.65 0.  ]\n",
      " [0.   0.2  0.   0.8 ]]\n",
      "Stationary distributions:\n",
      "[[ 0.33333333  0.          0.66666667  0.        ]\n",
      " [-0.          0.25       -0.          0.75      ]]\n",
      "Using [-0.00, 0.25, -0.00, 0.75] as initial distribution\n",
      "\n",
      "KL divergence of stationary distribution prefix of length     1 is 1.43505908\n",
      "KL divergence of stationary distribution prefix of length    10 is 0.34742443\n",
      "KL divergence of stationary distribution prefix of length   100 is 0.04198005\n",
      "KL divergence of stationary distribution prefix of length  1000 is 0.00326303\n",
      "KL divergence of stationary distribution prefix of length 10000 is 0.00000000\n"
     ]
    }
   ],
   "source": [
    "def array_to_dict(arr):\n",
    "    transition_dict = defaultdict(dict)\n",
    "    for key in 'ACGT':\n",
    "        transition_dict[key] = {k: v for k, v in zip('ACGT', arr[0])} \n",
    "        \n",
    "    return transition_dict\n",
    "\n",
    "def kl_divergences(initial, transition):\n",
    "    \"\"\"\n",
    "    Calculates the the Kullback-Leibler divergences between empirical distributions\n",
    "    generated using a markov model seeded with an initial distributin and a transition \n",
    "    matrix, and the initial distribution.\n",
    "    Sequences of length [1, 10, 100, 1000, 10000] are generated.\n",
    "    \"\"\"\n",
    "    KLs = []\n",
    "    initial_dict = dict(zip('ACGT', initial))\n",
    "    transition_dict = array_to_dict(transition)\n",
    "    \n",
    "    mc = MarkovChain(initial_dict, transition_dict, 1)\n",
    "    s = mc.generate(10000)\n",
    "    pseudo_prob = better_context_probabilities(s, 0)['']\n",
    "    \n",
    "    for l in [1, 10, 100, 1000, 10000]:\n",
    "        zeroth = better_context_probabilities(s[:l], 0)['']\n",
    "        KLs.append(kullback_leibler(pseudo_prob, zeroth))\n",
    "    \n",
    "    return zip([1, 10, 100, 1000, 10000], KLs)#np.random.rand(5))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    transition=np.array([[0.3, 0, 0.7, 0],\n",
    "                         [0, 0.4, 0, 0.6],\n",
    "                         [0.35, 0, 0.65, 0],\n",
    "                         [0, 0.2, 0, 0.8]])\n",
    "    print(\"Transition probabilities are:\")\n",
    "    print(transition)\n",
    "    stationary_distributions = get_stationary_distributions(transition)\n",
    "    print(\"Stationary distributions:\")\n",
    "    print(np.stack(stationary_distributions))\n",
    "    initial = stationary_distributions[1]\n",
    "    print(\"Using [{}] as initial distribution\\n\".format(\", \".join(f\"{v:.2f}\" for v in initial)))\n",
    "    results = kl_divergences(initial, transition)\n",
    "    for prefix_length, divergence in results: # iterate on prefix lengths in order (1, 10, 100...)\n",
    "        print(\"KL divergence of stationary distribution prefix \" \\\n",
    "              \"of length {:5d} is {:.8f}\".format(prefix_length, divergence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Idea of solution\n",
    "\n",
    "Convert initial and transition into distribution dictionaries. Create Markov chain with initial and transition distributions as ```zeroth``` and ```kth``` and ```k=1``` and generate chain of length 10000. Count pseudo-probabilities for chain. Loop though list of prefix length and count ```zeroth``` for prefix. Then count Kullback-Leibner for pseudo-probabilities and ```zeroth```. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "KL diverges towards zero as prefix length rises, which is expected. Again the description is so unclear it is impossible to know what actually is expected and analysing results is equally impossible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "22. Implement the following in the ```main``` function.\n",
    "Find the stationary distribution for the following transition matrix:  \n",
    "\n",
    "\\begin{array}{ l | r r r r}\n",
    " & A &     C &     G &     T \\\\\n",
    "\\hline\n",
    "A &  0.30 &  0.10 &  0.50 &  0.10 \\\\\n",
    "C &  0.20 &  0.30 &  0.15 &  0.35 \\\\\n",
    "G &  0.25 &  0.15 &  0.20 &  0.40 \\\\\n",
    "T &  0.35 &  0.20 &  0.40 &  0.05 \\\\\n",
    "\\end{array}\n",
    "\n",
    "Since there is only one stationary distribution, it is called the *equilibrium distribution*.\n",
    "Choose randomly two nucleotide distributions. You can take these from your sleeve or\n",
    "sample them from the Dirichlet distribution. Then for each of these distributions\n",
    "as the initial distribution of the Markov chain, repeat the above experiment.\n",
    "\n",
    "The `main` function should return tuples, where the first element is the (random) initial distribution and the second element contains the results as a list of tuples where the first element is the kl divergence and the second element the empirical nucleotide distribution, for the different prefix lengths.\n",
    "\n",
    "The state distribution should converge to the equilibrium distribution no matter how we\n",
    "start the Markov chain! That is the last line of the tables should have KL-divergence very close to $0$ and an empirical distribution very close to the equilibrium distribution.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1908,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-08T22:04:23.681300Z",
     "start_time": "2019-07-08T22:04:23.657345Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transition probabilities are:\n",
      "[[0.3  0.1  0.5  0.1 ]\n",
      " [0.2  0.3  0.15 0.35]\n",
      " [0.25 0.15 0.2  0.4 ]\n",
      " [0.35 0.2  0.4  0.05]]\n",
      "Equilibrium distribution:\n",
      "[0.27803345 0.17353238 0.32035021 0.22808396]\n",
      "\n",
      "Using [ 0.13160622 -0.30825875 -0.16930042  0.21806126] as initial distribution:\n",
      "kl-divergence   empirical distribution\n",
      "0.53401633558   [0.2 0.4 0.2 0.2]\n",
      "0.30317530920   [0.14285714 0.28571429 0.35714286 0.21428571]\n",
      "0.02444141704   [0.25       0.15384615 0.48076923 0.11538462]\n",
      "0.00020080879   [0.29581673 0.10258964 0.49900398 0.10258964]\n",
      "0.00000000000   [0.30197921 0.10095962 0.4980008  0.09906038]\n",
      "\n",
      "Using [ 0.02003038  0.21907796 -0.27786415  0.43512535] as initial distribution:\n",
      "kl-divergence   empirical distribution\n",
      "0.14279418370   [0.2 0.2 0.4 0.2]\n",
      "0.03199328909   [0.28571429 0.14285714 0.42857143 0.14285714]\n",
      "0.02010863409   [0.32692308 0.125      0.42307692 0.125     ]\n",
      "0.00624369516   [0.29183267 0.1185259  0.51095618 0.07868526]\n",
      "0.00000000000   [0.30327869 0.09816074 0.50079968 0.0977609 ]\n"
     ]
    }
   ],
   "source": [
    "def main(transition, equilibrium_distribution):\n",
    "    initials = [np.random.default_rng().dirichlet([10, 15, 50, 25], 1),\n",
    "                np.random.default_rng().dirichlet(np.random.random(4), 1)]\n",
    "    KLs = []\n",
    "    distributions = []\n",
    "    #equilibrium_dict = dict(zip('ACGT', equilibrium_distribution))\n",
    "    transition_dict = array_to_dict(transition)\n",
    "    for initial in initials:\n",
    "        mc = MarkovChain(dict(zip('ACGT', initial.flatten())), transition_dict, 1)\n",
    "        s = mc.generate(10000)\n",
    "        pseudo_probs = better_context_probabilities(s, 0)['']\n",
    "        #print('pseudo', pseudo_probs)\n",
    "        for l in [1, 10, 100, 1000, 10000]:\n",
    "            zeroth = better_context_probabilities(s[:l], 0)['']\n",
    "            KLs.append(kullback_leibler(pseudo_probs, zeroth))\n",
    "            distributions.append(np.fromiter(zeroth.values(), dtype=float))\n",
    "    \n",
    "    \n",
    "    #vals = list(zip(np.random.rand(10), np.random.rand(10, 4) - 0.5))\n",
    "    vals = list(zip(KLs, distributions))\n",
    "    return zip(np.random.rand(2, 4) - 0.5,#initials, #np.random.rand(2, 4) - 0.5, \n",
    "               [vals[:5], vals[5:]])\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    transition = np.array([[0.3, 0.1, 0.5, 0.1],\n",
    "                           [0.2, 0.3, 0.15, 0.35],\n",
    "                           [0.25, 0.15, 0.2, 0.4],\n",
    "                           [0.35, 0.2, 0.4, 0.05]])\n",
    "    print(\"Transition probabilities are:\", transition, sep=\"\\n\")\n",
    "    stationary_distributions = get_stationary_distributions(transition)\n",
    "    # Uncomment the below line to check that there actually is only one stationary distribution\n",
    "    assert len(stationary_distributions) == 1\n",
    "    equilibrium_distribution = stationary_distributions[0]\n",
    "    print(\"Equilibrium distribution:\")\n",
    "    print(equilibrium_distribution)\n",
    "    for initial_distribution, results in main(transition, equilibrium_distribution):\n",
    "        print(\"\\nUsing {} as initial distribution:\".format(initial_distribution))\n",
    "        print(\"kl-divergence   empirical distribution\")\n",
    "        print(\"\\n\".join(\"{:.11f}   {}\".format(di, kl) for di, kl in results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Idea of solution\n",
    "\n",
    "Idea is similar to previous exercise. Though it is unclear where to use ```equilibrium_distribution```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "Distributions won't converge to equilibrium and tests won't pass. Lot's of guessing was required to find out what data types the result should have. The instructions are quite useless. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "598.85px",
    "left": "1223px",
    "right": "20px",
    "top": "121px",
    "width": "353px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
